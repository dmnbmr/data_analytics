{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andres Ambuehl plays for HC Davos. Denis Malgin plays for ZSC Lions. Austin Czarnik plays for SC Bern.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents (=sentenses)\n",
    "d1 = 'Andres Ambuehl plays for HC Davos.'\n",
    "d2 = 'Denis Malgin plays for ZSC Lions.'\n",
    "d3 = 'Austin Czarnik plays for SC Bern.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andres ambuehl plays for hc davos. denis malgin plays for zsc lions. austin czarnik plays for sc bern.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'andres ambuehl plays for hc davos denis malgin plays for zsc lions austin czarnik plays for sc bern'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{'below', 'mustn', \"isn't\", 'mightn', 'himself', 'other', 'just', 'you', 'that', 'off', 'further', 'yourselves', 'each', 'at', 'hasn', \"wasn't\", 'such', 'wasn', 'this', 'before', 'ma', \"that'll\", 'ain', 'hadn', \"should've\", 'again', 'isn', 'nor', 'its', 'be', 'can', \"you'd\", 'ours', 'i', 'our', 'they', 'not', 'won', 'yourself', \"hadn't\", \"you're\", 'ourselves', 'into', 'we', 'after', \"couldn't\", 'm', \"doesn't\", 'wouldn', 's', 'he', 'her', 'haven', 'these', \"she's\", 'doesn', 'should', 'too', 'are', 'll', 'had', 'o', 'if', 'me', 'will', 'it', 'itself', 'which', 'from', \"shouldn't\", 'herself', 'd', 'above', 'when', 'once', 'whom', \"shan't\", 'but', 'she', 'there', 'those', 'shan', 'against', 'what', 'needn', 't', \"needn't\", 'only', 're', 'does', 've', 'the', 'their', 'while', \"you'll\", 'few', 'been', \"aren't\", 'to', 'his', 'over', 'between', 'all', 'until', 'in', 'a', 'weren', 'or', \"wouldn't\", 'doing', 'of', 'both', \"hasn't\", 'y', 'because', 'why', 'up', 'them', 'so', 'same', 'an', 'down', 'don', 'under', 'did', \"weren't\", \"you've\", \"mightn't\", \"mustn't\", 'having', 'now', 'during', 'couldn', 'more', 'any', \"don't\", 'very', 'by', 'do', \"haven't\", 'with', 'themselves', 'who', \"didn't\", 'theirs', 'on', 'through', 'myself', 'about', 'some', 'hers', 'your', 'here', 'has', 'no', 'own', \"it's\", 'how', 'as', 'out', 'was', 'most', 'then', 'and', 'than', 'have', 'my', 'him', 'being', 'didn', 'shouldn', 'yours', \"won't\", 'is', 'where', 'am', 'aren', 'for', 'were'}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['andres', 'ambuehl', 'plays', 'hc', 'davos', 'denis', 'malgin', 'plays', 'zsc', 'lions', 'austin', 'czarnik', 'plays', 'sc', 'bern']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['andres', 'ambuehl', 'plays', 'hc', 'davos', 'denis', 'malgin', 'plays', 'zsc', 'lions', 'austin', 'czarnik', 'plays', 'sc', 'bern'] \n",
      "\n",
      "After lemmatization:\n",
      "['andres', 'ambuehl', 'play', 'hc', 'davos', 'denis', 'malgin', 'play', 'zsc', 'lions', 'austin', 'czarnik', 'play', 'sc', 'bern']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the lemmatized words above to re-define our corpus \n",
    "corpus = ['andres ambuehl play hc davos', \n",
    "          'denis malgin play zsc lion', \n",
    "          'austin czarnik play sc bern']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   ambuehl  andres  austin  bern  czarnik  davos  denis  hc  lion  malgin  \\\n",
      "0        1       1       0     0        0      1      0   1     0       0   \n",
      "1        0       0       0     0        0      0      1   0     1       1   \n",
      "2        0       0       1     1        1      0      0   0     0       0   \n",
      "\n",
      "   play  sc  zsc  \n",
      "0     1   0    0  \n",
      "1     1   0    1  \n",
      "2     1   1    0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   ambuehl play  andres ambuehl  austin czarnik  czarnik play  denis malgin  \\\n",
      "0             1               1               0             0             0   \n",
      "1             0               0               0             0             1   \n",
      "2             0               0               1             1             0   \n",
      "\n",
      "   hc davos  malgin play  play hc  play sc  play zsc  sc bern  zsc lion  \n",
      "0         1            0        1        0         0        0         0  \n",
      "1         0            1        0        0         1        0         1  \n",
      "2         0            0        0        1         0        1         0  \n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 13 \n",
      "\n",
      "The words in the corpus: \n",
      " {'lion', 'bern', 'sc', 'czarnik', 'ambuehl', 'austin', 'malgin', 'davos', 'andres', 'denis', 'hc', 'play', 'zsc'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "   lion  bern   sc  czarnik  ambuehl  austin  malgin  davos  andres  denis  \\\n",
      "0   0.0   0.0  0.0      0.0      0.2     0.0     0.0    0.2     0.2    0.0   \n",
      "1   0.2   0.0  0.0      0.0      0.0     0.0     0.2    0.0     0.0    0.2   \n",
      "2   0.0   0.2  0.2      0.2      0.0     0.2     0.0    0.0     0.0    0.0   \n",
      "\n",
      "    hc  play  zsc  \n",
      "0  0.2   0.2  0.0  \n",
      "1  0.0   0.2  0.2  \n",
      "2  0.0   0.2  0.0  \n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "           lion:     0.4771\n",
      "           bern:     0.4771\n",
      "             sc:     0.4771\n",
      "        czarnik:     0.4771\n",
      "        ambuehl:     0.4771\n",
      "         austin:     0.4771\n",
      "         malgin:     0.4771\n",
      "          davos:     0.4771\n",
      "         andres:     0.4771\n",
      "          denis:     0.4771\n",
      "             hc:     0.4771\n",
      "           play:        0.0\n",
      "            zsc:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "     lion    bern      sc  czarnik  ambuehl  austin  malgin   davos  andres  \\\n",
      "0  0.0000  0.0000  0.0000   0.0000   0.0954  0.0000  0.0000  0.0954  0.0954   \n",
      "1  0.0954  0.0000  0.0000   0.0000   0.0000  0.0000  0.0954  0.0000  0.0000   \n",
      "2  0.0000  0.0954  0.0954   0.0954   0.0000  0.0954  0.0000  0.0000  0.0000   \n",
      "\n",
      "    denis      hc  play     zsc  \n",
      "0  0.0000  0.0954   0.0  0.0000  \n",
      "1  0.0954  0.0000   0.0  0.0954  \n",
      "2  0.0000  0.0000   0.0  0.0000  \n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Andres', 'NNS', 'O'),\n",
      " ('Ambuehl', 'NNP', 'O'),\n",
      " ('plays', 'NNS', 'O'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('HC', 'NNP', 'O'),\n",
      " ('Davos', 'NNP', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('Swiss', 'JJ', 'I-NP'),\n",
      " ('national', 'JJ', 'I-NP'),\n",
      " ('team', 'NN', 'I-NP'),\n",
      " ('.', '.', 'O'),\n",
      " ('He', 'PRP', 'O'),\n",
      " ('is', 'VBZ', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('record', 'NN', 'I-NP'),\n",
      " ('player', 'NN', 'B-NP'),\n",
      " ('for', 'IN', 'O'),\n",
      " ('both', 'DT', 'O'),\n",
      " ('teams', 'NNS', 'O'),\n",
      " ('.', '.', 'O'),\n",
      " ('In', 'IN', 'O'),\n",
      " ('both', 'DT', 'O'),\n",
      " ('teams', 'NNS', 'O'),\n",
      " ('he', 'PRP', 'O'),\n",
      " ('has', 'VBZ', 'O'),\n",
      " ('played', 'VBN', 'O'),\n",
      " ('more', 'RBR', 'O'),\n",
      " ('games', 'NNS', 'O'),\n",
      " ('than', 'IN', 'O'),\n",
      " ('anybody', 'NN', 'B-NP'),\n",
      " ('else', 'RB', 'O'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = '''Andres Ambuehl plays for HC Davos and the Swiss national team. He is the record player for both teams. In both teams he has played more games than anybody else.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07fceab",
   "metadata": {},
   "source": [
    "NNP is a Proper Noun in singular.\n",
    "\n",
    "NNS is a Noun in singular.\n",
    "\n",
    "DT is a Determiner.\n",
    "\n",
    "IN is a Preposition/Subordinating Conjunction.\n",
    "\n",
    "PRP is a Personal Pronoun.\n",
    "\n",
    "RBR is an Adverb in the comparative form."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.5.0-1025-azure\n",
      "Datetime: 2024-11-04 12:25:10\n",
      "Python Version: 3.11.10\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
